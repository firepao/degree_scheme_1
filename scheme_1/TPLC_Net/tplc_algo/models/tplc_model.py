from __future__ import annotations

from dataclasses import dataclass
from typing import List

import torch
import torch.nn as nn
import torch.nn.functional as F

from .layers import (
    DepthwiseSeparableConv2d,
    extract_topk_periods,
    reshape_1d_to_2d,
    reshape_2d_to_1d,
)


class MultiScaleGenerator(nn.Module):
    """å¤šå°ºåº¦åºåˆ—ç”Ÿæˆï¼šä½¿ç”¨å¹³å‡æ± åŒ–å®ç° stride=2 çš„ä¸‹é‡‡æ ·ã€‚"""

    def __init__(self, num_scales: int) -> None:
        super().__init__()
        self.num_scales = int(num_scales)

    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:
        # x: [B, C, T]
        scales = [x]
        cur = x
        for _ in range(self.num_scales):
            # å¹³å‡æ± åŒ–ä¸‹é‡‡æ ·ï¼Œä¿æŒå˜é‡ä¸€è‡´
            cur = F.avg_pool1d(cur, kernel_size=2, stride=2, ceil_mode=False)
            scales.append(cur)
        return scales


class TPLCNet(nn.Module):
    """TPLCNetï¼šå¤šå°ºåº¦ + FFT å‘¨æœŸè¯†åˆ« + 1Dâ†’2D + æ·±åº¦å¯åˆ†ç¦»å·ç§¯ + å¤šå‘¨æœŸ/å¤šå°ºåº¦èåˆã€‚"""

    def __init__(
        self,
        input_dim: int,
        target_dim: int,
        seq_len: int,
        pred_len: int,
        num_scales: int = 2,
        top_k_periods: int = 3,
        hidden_dim: int = 64,
        dw_kernel: int = 3,
    ) -> None:
        super().__init__()
        self.input_dim = int(input_dim)
        self.target_dim = int(target_dim)
        self.seq_len = int(seq_len)
        self.pred_len = int(pred_len)
        self.num_scales = int(num_scales)
        self.top_k_periods = int(top_k_periods)
        self.hidden_dim = int(hidden_dim)

        self.multi_scale = MultiScaleGenerator(num_scales=self.num_scales)
        self.conv2d = DepthwiseSeparableConv2d(
            in_channels=self.input_dim,
            out_channels=self.hidden_dim,
            kernel_size=dw_kernel,
        )

        # å¤šå°ºåº¦ headï¼šæ¯ä¸ªå°ºåº¦ä¸€ä¸ªâ€œæ—¶é—´æ˜ å°„çº¿æ€§å±‚â€ï¼Œå†å…±äº«ä¸€ä¸ªâ€œé€šé“æ˜ å°„çº¿æ€§å±‚â€
        self.time_projs = nn.ModuleList()
        for m in range(self.num_scales + 1):
            l_m = self.seq_len // (2**m)
            l_m = max(1, l_m)
            self.time_projs.append(nn.Linear(l_m, self.pred_len))
        self.channel_proj = nn.Linear(self.hidden_dim, self.target_dim)

        # å¤šå°ºåº¦èåˆæƒé‡ï¼šsoftmax åå†ä¹˜ä»¥ (num_scales+1)ï¼Œä½¿åˆå§‹åŒ–ç­‰ä»·äºâ€œç›´æ¥ç›¸åŠ â€
        self.scale_logits = nn.Parameter(torch.zeros(self.num_scales + 1))        
        # Dropout å±‚ç”¨äºæ­£åˆ™åŒ–ï¼ˆé»˜è®¤ 0.1ï¼‰
        self.dropout = nn.Dropout(p=0.1)
        
        # æ®‹å·®æŠ•å½±å±‚ï¼ˆå¦‚æœè¾“å…¥ç»´åº¦ä¸è¾“å‡ºç»´åº¦ä¸åŒ¹é…ï¼‰
        self.residual_proj = None
        if self.input_dim != self.target_dim:
            self.residual_proj = nn.Linear(self.input_dim, self.target_dim)
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ã€‚

        å‚æ•°
        - x: [B, seq_len, input_dim]
        è¿”å›
        - y_hat: [B, pred_len, target_dim]
        """

        if x.dim() != 3:
            raise ValueError("x å¿…é¡»æ˜¯ [B, T, C]")
        b, t, c = x.shape
        if t != self.seq_len:
            raise ValueError(f"seq_len ä¸åŒ¹é…ï¼šè¾“å…¥ T={t}, æœŸæœ› {self.seq_len}")
        if c != self.input_dim:
            raise ValueError(f"input_dim ä¸åŒ¹é…ï¼šè¾“å…¥ C={c}, æœŸæœ› {self.input_dim}")

        # ğŸ”§ ä¿å­˜åŸå§‹è¾“å…¥ç”¨äºæ®‹å·®è¿æ¥
        x_residual = x  # [B, seq_len, input_dim]
        
        x_ct = x.transpose(1, 2).contiguous()  # [B, C, T]
        scale_list = self.multi_scale(x_ct)

        # [M]ï¼ŒM=num_scales+1ï¼›åˆå§‹åŒ–æ—¶æ¯ä¸ªå°ºåº¦æƒé‡ä¸º 1ï¼ˆä¸æ—§ç‰ˆç›´æ¥ç›¸åŠ ä¸€è‡´ï¼‰
        m_scales = self.num_scales + 1
        scale_weights = torch.softmax(self.scale_logits, dim=0) * float(m_scales)

        y_sum = None
        for m, x_m in enumerate(scale_list):
            # x_m: [B, C, Lm]
            period_info = extract_topk_periods(x_m, top_k=self.top_k_periods)
            periods = period_info.periods  # [K]
            amps = period_info.amplitudes  # [K]
            weights = torch.softmax(amps, dim=0)  # [K]

            feats_1d = []
            for k in range(self.top_k_periods):
                p = int(periods[k].item())
                z2d, orig_len = reshape_1d_to_2d(x_m, period=p)  # [B,C,rows,p]
                y2d = self.conv2d(z2d)  # [B, hidden, rows, p]
                y1d = reshape_2d_to_1d(y2d, orig_len=orig_len)  # [B, hidden, Lm]
                feats_1d.append(y1d)

            # å¤šå‘¨æœŸèåˆï¼šx_m^l = sum_k softmax(A_k) * X_{m,k}
            fused = torch.zeros_like(feats_1d[0])
            for k in range(self.top_k_periods):
                fused = fused + weights[k] * feats_1d[k]
            
            # ğŸ”§ æ–°å¢ï¼šåº”ç”¨ Dropout
            fused = self.dropout(fused)

            # é¢„æµ‹å¤´ï¼šæ—¶é—´ç»´ Lm -> pred_lenï¼Œå† hidden -> target_dim
            # fused: [B, hidden, Lm]
            l_m = fused.shape[-1]
            # time_proj_m æœŸæœ›è¾“å…¥é•¿åº¦ä¸æ„é€ æ—¶ä¸€è‡´ï¼›è‹¥å› æ± åŒ–å¯¼è‡´ Lm ä¸æ•´é™¤ä¸ä¸€è‡´ï¼Œåšæˆªæ–­/è¡¥é›¶
            expected_lm = self.time_projs[m].in_features
            if l_m > expected_lm:
                fused_use = fused[..., -expected_lm:]
            elif l_m < expected_lm:
                fused_use = F.pad(fused, (expected_lm - l_m, 0), value=0.0)
            else:
                fused_use = fused

            # [B, hidden, pred]
            pred_hidden = self.time_projs[m](fused_use)
            pred_hidden = pred_hidden.transpose(1, 2).contiguous()  # [B, pred, hidden]
            pred_hidden = self.dropout(pred_hidden)  # ğŸ”§ æ–°å¢ï¼šDropout
            y_m = self.channel_proj(pred_hidden)  # [B, pred, target]
            w_m = scale_weights[m]
            y_sum = (w_m * y_m) if y_sum is None else (y_sum + w_m * y_m)

        assert y_sum is not None
        
        # ğŸ”§ æ–°å¢ï¼šæ®‹å·®è¿æ¥ï¼ˆTimesNet é£æ ¼ï¼‰
        # å–è¾“å…¥åºåˆ—çš„å pred_len æ­¥ä½œä¸ºæ®‹å·®åŸºå‡†
        if self.seq_len >= self.pred_len:
            x_res = x_residual[:, -self.pred_len:, :]  # [B, pred_len, input_dim]
        else:
            # å¦‚æœåºåˆ—é•¿åº¦ä¸è¶³ï¼Œç”¨é›¶å¡«å……
            x_res = F.pad(x_residual, (0, 0, self.pred_len - self.seq_len, 0))[:, :self.pred_len, :]
        
        # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨æŠ•å½±å±‚
        if self.residual_proj is not None:
            x_res = self.residual_proj(x_res)  # [B, pred_len, target_dim]
        
        # æ®‹å·®è¿æ¥
        y_sum = y_sum + x_res
        
        return y_sum
